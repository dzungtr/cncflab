ollama:
  # -- Automatically install Ollama Helm chart from https://otwld.github.io/ollama-helm/. Use [Helm Values](https://github.com/otwld/ollama-helm/#helm-values) to configure
  enabled: true
  # -- If enabling embedded Ollama, update fullnameOverride to your desired Ollama name value, or else it will use the default ollama.name value from the Ollama chart
  fullnameOverride: "open-webui-ollama"
  # -- Example Ollama configuration with nvidia GPU enabled, automatically downloading a model, and deploying a PVC for model persistence
  ollama:
    gpu:
      enabled: true
      type: 'nvidia'
      number: 1
    models:
      pull:
        - gemma2:9b
        - gemma3:12b-it-q8_0
      run:
        - gemma2:9b
  image:
    # -- Docker image registry
    repository: ollama/ollama
    # -- Docker image tag, overrides the image tag whose default is the chart appVersion.
    tag: "0.6.8"
  runtimeClassName: nvidia
  persistentVolume:
    enabled: true
    size: 30Gi
  extraEnv:
    - name: OLLAMA_LLM_LIBRARY
      value: cuda

extraEnvVars:
  - name: ENABLE_OLLAMA_API
    value: "true"
  - name: OLLAMA_BASE_URLS
    value: http://open-webui-ollama:11434
